{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7ce83bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:07:06.959143Z",
     "iopub.status.busy": "2025-07-10T07:07:06.958851Z",
     "iopub.status.idle": "2025-07-10T07:07:06.965013Z",
     "shell.execute_reply": "2025-07-10T07:07:06.964173Z",
     "shell.execute_reply.started": "2025-07-10T07:07:06.959119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from timeit import Timer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4993cfd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:11:58.876021Z",
     "iopub.status.busy": "2025-07-10T07:11:58.875696Z",
     "iopub.status.idle": "2025-07-10T07:11:59.048982Z",
     "shell.execute_reply": "2025-07-10T07:11:59.048138Z",
     "shell.execute_reply.started": "2025-07-10T07:11:58.875991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/geoelectric-models/geoelectric_models.csv\")\n",
    "X = df[[f'dBzdt_{i}' for i in range(1, 41)]].values\n",
    "y = df[[f'rho_{i}' for i in range(1, 21)] + [f'thk_{i}' for i in range(1, 20)]].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print(X.shape)\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10686c02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:12:41.555163Z",
     "iopub.status.busy": "2025-07-10T07:12:41.554832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 380.196655\n",
      "Epoch 1000: Loss = 380.196655\n",
      "Epoch 2000: Loss = 380.196655\n",
      "Epoch 3000: Loss = 380.196655\n",
      "Epoch 4000: Loss = 380.196655\n",
      "Epoch 5000: Loss = 380.196655\n",
      "Epoch 6000: Loss = 380.196655\n",
      "Epoch 7000: Loss = 380.196655\n",
      "Epoch 8000: Loss = 380.196655\n",
      "Epoch 9000: Loss = 380.196655\n"
     ]
    }
   ],
   "source": [
    "class NLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(40, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 39)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = NLP()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "def train_model(model,EPOCHS = 100):\n",
    "    # EPOCHS = 100\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        y_pred = model(X_train_t)\n",
    "        loss = loss_fn(y_pred, y_train_t)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch  % 1000 == 0 :\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
    "train_model(model, EPOCHS=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b1e349e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:24:28.770274Z",
     "iopub.status.busy": "2025-07-10T07:24:28.769978Z",
     "iopub.status.idle": "2025-07-10T07:31:11.025046Z",
     "shell.execute_reply": "2025-07-10T07:31:11.024291Z",
     "shell.execute_reply.started": "2025-07-10T07:24:28.770251Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | loss_train: 380.2528076171875 | loss_test: 381.0336608886719\n",
      "epoch:100 | loss_train: 188.6985321044922 | loss_test: 189.80702209472656\n",
      "epoch:200 | loss_train: 187.15835571289062 | loss_test: 188.6143341064453\n",
      "epoch:300 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:400 | loss_train: 187.1582794189453 | loss_test: 188.6155242919922\n",
      "epoch:500 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:600 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:700 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:800 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:900 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:1000 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:1100 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:1200 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:1400 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:1500 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:1600 | loss_train: 187.15826416015625 | loss_test: 188.61549377441406\n",
      "epoch:1700 | loss_train: 187.15826416015625 | loss_test: 188.61549377441406\n",
      "epoch:1800 | loss_train: 187.1582794189453 | loss_test: 188.61549377441406\n",
      "epoch:1900 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:2000 | loss_train: 187.1582794189453 | loss_test: 188.61550903320312\n",
      "epoch:2100 | loss_train: 187.1582489013672 | loss_test: 188.61546325683594\n",
      "epoch:2200 | loss_train: 187.15826416015625 | loss_test: 188.61549377441406\n",
      "epoch:2300 | loss_train: 187.1582489013672 | loss_test: 188.61549377441406\n",
      "epoch:2400 | loss_train: 187.1582489013672 | loss_test: 188.6154327392578\n",
      "epoch:2500 | loss_train: 187.1582489013672 | loss_test: 188.61549377441406\n",
      "epoch:2600 | loss_train: 187.1582489013672 | loss_test: 188.61546325683594\n",
      "epoch:2700 | loss_train: 187.1582794189453 | loss_test: 188.615478515625\n",
      "epoch:2800 | loss_train: 187.1582489013672 | loss_test: 188.6154327392578\n",
      "epoch:2900 | loss_train: 187.15826416015625 | loss_test: 188.61544799804688\n",
      "epoch:3000 | loss_train: 187.1582489013672 | loss_test: 188.61549377441406\n",
      "epoch:3100 | loss_train: 187.1582489013672 | loss_test: 188.6154022216797\n",
      "epoch:3200 | loss_train: 187.1582489013672 | loss_test: 188.6154022216797\n",
      "epoch:3300 | loss_train: 187.1582489013672 | loss_test: 188.61546325683594\n",
      "epoch:3400 | loss_train: 187.1582489013672 | loss_test: 188.61541748046875\n",
      "epoch:3500 | loss_train: 187.1582489013672 | loss_test: 188.6153564453125\n",
      "epoch:3600 | loss_train: 187.1582489013672 | loss_test: 188.61541748046875\n",
      "epoch:3700 | loss_train: 187.1582489013672 | loss_test: 188.6154022216797\n",
      "epoch:3800 | loss_train: 187.1582489013672 | loss_test: 188.61541748046875\n",
      "epoch:3900 | loss_train: 187.1582489013672 | loss_test: 188.6153564453125\n",
      "epoch:4000 | loss_train: 187.1582489013672 | loss_test: 188.61537170410156\n",
      "epoch:4100 | loss_train: 187.15823364257812 | loss_test: 188.61541748046875\n",
      "epoch:4200 | loss_train: 187.15823364257812 | loss_test: 188.6153564453125\n",
      "epoch:4300 | loss_train: 187.1582489013672 | loss_test: 188.61532592773438\n",
      "epoch:4400 | loss_train: 187.1582489013672 | loss_test: 188.61532592773438\n",
      "epoch:4500 | loss_train: 187.1582489013672 | loss_test: 188.61532592773438\n",
      "epoch:4600 | loss_train: 187.1582489013672 | loss_test: 188.61524963378906\n",
      "epoch:4700 | loss_train: 187.15823364257812 | loss_test: 188.61532592773438\n",
      "epoch:4800 | loss_train: 187.1582489013672 | loss_test: 188.615234375\n",
      "epoch:4900 | loss_train: 187.1582489013672 | loss_test: 188.61512756347656\n",
      "epoch:5000 | loss_train: 187.15823364257812 | loss_test: 188.61538696289062\n",
      "epoch:5100 | loss_train: 187.1582489013672 | loss_test: 188.6152801513672\n",
      "epoch:5200 | loss_train: 187.1582489013672 | loss_test: 188.6153106689453\n",
      "epoch:5300 | loss_train: 187.1582489013672 | loss_test: 188.61541748046875\n",
      "epoch:5400 | loss_train: 187.15823364257812 | loss_test: 188.6153564453125\n",
      "epoch:5500 | loss_train: 187.15823364257812 | loss_test: 188.61532592773438\n",
      "epoch:5600 | loss_train: 187.15823364257812 | loss_test: 188.61561584472656\n",
      "epoch:5700 | loss_train: 187.15823364257812 | loss_test: 188.61557006835938\n",
      "epoch:5800 | loss_train: 187.15823364257812 | loss_test: 188.61541748046875\n",
      "epoch:5900 | loss_train: 187.15823364257812 | loss_test: 188.61558532714844\n",
      "epoch:6000 | loss_train: 187.1582489013672 | loss_test: 188.61509704589844\n",
      "epoch:6100 | loss_train: 187.15823364257812 | loss_test: 188.6153106689453\n",
      "epoch:6200 | loss_train: 187.15823364257812 | loss_test: 188.61561584472656\n",
      "epoch:6300 | loss_train: 187.15823364257812 | loss_test: 188.61550903320312\n",
      "epoch:6400 | loss_train: 187.15823364257812 | loss_test: 188.615478515625\n",
      "epoch:6500 | loss_train: 187.15834045410156 | loss_test: 188.61473083496094\n",
      "epoch:6600 | loss_train: 187.1582489013672 | loss_test: 188.6158447265625\n",
      "epoch:6700 | loss_train: 187.15902709960938 | loss_test: 188.61325073242188\n",
      "epoch:6800 | loss_train: 187.15866088867188 | loss_test: 188.6138153076172\n",
      "epoch:6900 | loss_train: 187.1582794189453 | loss_test: 188.61810302734375\n",
      "epoch:7000 | loss_train: 187.1587371826172 | loss_test: 188.61264038085938\n",
      "epoch:7100 | loss_train: 187.15870666503906 | loss_test: 188.61380004882812\n",
      "epoch:7200 | loss_train: 187.15830993652344 | loss_test: 188.61795043945312\n",
      "epoch:7300 | loss_train: 187.1587677001953 | loss_test: 188.61248779296875\n",
      "epoch:7400 | loss_train: 187.15872192382812 | loss_test: 188.61376953125\n",
      "epoch:7500 | loss_train: 187.1582794189453 | loss_test: 188.618408203125\n",
      "epoch:7600 | loss_train: 187.1588134765625 | loss_test: 188.61253356933594\n",
      "epoch:7700 | loss_train: 187.15870666503906 | loss_test: 188.61460876464844\n",
      "epoch:7800 | loss_train: 187.1582489013672 | loss_test: 188.61851501464844\n",
      "epoch:7900 | loss_train: 187.15899658203125 | loss_test: 188.6121368408203\n",
      "epoch:8000 | loss_train: 187.15902709960938 | loss_test: 188.61534118652344\n",
      "epoch:8100 | loss_train: 187.15835571289062 | loss_test: 188.6184539794922\n",
      "epoch:8200 | loss_train: 187.15899658203125 | loss_test: 188.61203002929688\n",
      "epoch:8300 | loss_train: 187.15878295898438 | loss_test: 188.61471557617188\n",
      "epoch:8400 | loss_train: 187.1583251953125 | loss_test: 188.61776733398438\n",
      "epoch:8500 | loss_train: 187.15895080566406 | loss_test: 188.612548828125\n",
      "epoch:8600 | loss_train: 187.15882873535156 | loss_test: 188.61428833007812\n",
      "epoch:8700 | loss_train: 187.1582489013672 | loss_test: 188.61851501464844\n",
      "epoch:8800 | loss_train: 187.15866088867188 | loss_test: 188.61253356933594\n",
      "epoch:8900 | loss_train: 187.1591033935547 | loss_test: 188.6136016845703\n",
      "epoch:9000 | loss_train: 187.15826416015625 | loss_test: 188.61866760253906\n",
      "epoch:9100 | loss_train: 187.1587677001953 | loss_test: 188.61245727539062\n",
      "epoch:9200 | loss_train: 187.1590118408203 | loss_test: 188.61456298828125\n",
      "epoch:9300 | loss_train: 187.1582489013672 | loss_test: 188.61767578125\n",
      "epoch:9400 | loss_train: 187.15908813476562 | loss_test: 188.61305236816406\n",
      "epoch:9500 | loss_train: 187.1590576171875 | loss_test: 188.61512756347656\n",
      "epoch:9600 | loss_train: 187.15834045410156 | loss_test: 188.61761474609375\n",
      "epoch:9700 | loss_train: 187.1590576171875 | loss_test: 188.61248779296875\n",
      "epoch:9800 | loss_train: 187.15869140625 | loss_test: 188.61415100097656\n"
     ]
    }
   ],
   "source": [
    "act = nn.ReLU()\n",
    "\n",
    "class Model_comp(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ll1 = nn.Linear(40,128)\n",
    "    self.ac1 = nn.ReLU()\n",
    "    self.ll2 = nn.Linear(128,128)\n",
    "    self.ac2 = nn.ReLU()\n",
    "    self.ll3 = nn.Linear(128,128)\n",
    "    self.ac3 = nn.ReLU()\n",
    "    self.ll4 = nn.Linear(128,128)\n",
    "    self.ac4 = nn.ReLU()\n",
    "    self.output = nn.Linear(128,39)\n",
    "\n",
    "  def forward(self,X):\n",
    "    X = self.ll1(X)\n",
    "    X = self.ac1(X)\n",
    "    X = self.ll2(X)\n",
    "    X = self.ac2(X)\n",
    "    X = self.ll3(X)\n",
    "    X = self.ac3(X)\n",
    "    X = self.ll4(X)\n",
    "    X = self.ac4(X)\n",
    "    X = self.output(X)\n",
    "    return(X)\n",
    "\n",
    "dense_model = Model_comp()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(dense_model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "  dense_model.train()\n",
    "  y_pred = dense_model(X_train_t)\n",
    "  loss_train = loss_fn(y_pred,y_train_t)\n",
    "  optimizer.zero_grad()\n",
    "  loss_train.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  dense_model.eval()\n",
    "  with torch.inference_mode():\n",
    "    y_pred_test = dense_model(X_test_t)\n",
    "    loss_test = loss_fn(y_pred_test,y_test_t)\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    print(f\"epoch:{epoch} | loss_train: {loss_train} | loss_test: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84e57dae-8d99-4d18-a276-c5c27a09b949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:38:48.417203Z",
     "iopub.status.busy": "2025-07-10T07:38:48.416863Z",
     "iopub.status.idle": "2025-07-10T07:55:32.769999Z",
     "shell.execute_reply": "2025-07-10T07:55:32.768952Z",
     "shell.execute_reply.started": "2025-07-10T07:38:48.417175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Train MSE: 211178.673688 Val MSE: 212271.139125 LR: 1.00e-04\n",
      "Epoch 002 Train MSE: 209671.966750 Val MSE: 210659.185250 LR: 1.00e-04\n",
      "Epoch 003 Train MSE: 207952.227500 Val MSE: 208831.685875 LR: 1.00e-04\n",
      "Epoch 004 Train MSE: 206031.879750 Val MSE: 206809.183875 LR: 1.00e-04\n",
      "Epoch 005 Train MSE: 203919.715875 Val MSE: 204602.283875 LR: 1.00e-04\n",
      "Epoch 006 Train MSE: 201633.045188 Val MSE: 202226.902500 LR: 1.00e-04\n",
      "Epoch 007 Train MSE: 199188.704500 Val MSE: 199703.439250 LR: 1.00e-04\n",
      "Epoch 008 Train MSE: 196598.860500 Val MSE: 197038.295750 LR: 1.00e-04\n",
      "Epoch 009 Train MSE: 193876.802469 Val MSE: 194247.964375 LR: 1.00e-04\n",
      "Epoch 010 Train MSE: 191036.221562 Val MSE: 191343.950750 LR: 1.00e-04\n",
      "Epoch 011 Train MSE: 188086.801781 Val MSE: 188335.025875 LR: 1.00e-04\n",
      "Epoch 012 Train MSE: 185040.475406 Val MSE: 185238.787250 LR: 1.00e-04\n",
      "Epoch 013 Train MSE: 181908.148719 Val MSE: 182059.294125 LR: 1.00e-04\n",
      "Epoch 014 Train MSE: 178698.974875 Val MSE: 178804.295750 LR: 1.00e-04\n",
      "Epoch 015 Train MSE: 175420.426563 Val MSE: 175486.674875 LR: 1.00e-04\n",
      "Epoch 016 Train MSE: 172082.902000 Val MSE: 172114.501062 LR: 1.00e-04\n",
      "Epoch 017 Train MSE: 168695.232750 Val MSE: 168694.565000 LR: 1.00e-04\n",
      "Epoch 018 Train MSE: 165265.693250 Val MSE: 165234.778313 LR: 1.00e-04\n",
      "Epoch 019 Train MSE: 161801.265969 Val MSE: 161746.790062 LR: 1.00e-04\n",
      "Epoch 020 Train MSE: 158309.344219 Val MSE: 158236.936937 LR: 1.00e-04\n",
      "Epoch 021 Train MSE: 154796.350188 Val MSE: 154707.951063 LR: 1.00e-04\n",
      "Epoch 022 Train MSE: 151271.022156 Val MSE: 151168.530125 LR: 1.00e-04\n",
      "Epoch 023 Train MSE: 147741.381500 Val MSE: 147628.915500 LR: 1.00e-04\n",
      "Epoch 025 Train MSE: 140689.112063 Val MSE: 140562.755375 LR: 1.00e-04\n",
      "Epoch 026 Train MSE: 137180.755719 Val MSE: 137053.196625 LR: 1.00e-04\n",
      "Epoch 027 Train MSE: 133692.018156 Val MSE: 133565.933250 LR: 1.00e-04\n",
      "Epoch 028 Train MSE: 130229.925844 Val MSE: 130109.794313 LR: 1.00e-04\n",
      "Epoch 029 Train MSE: 126800.881437 Val MSE: 126685.507187 LR: 1.00e-04\n",
      "Epoch 030 Train MSE: 123410.628031 Val MSE: 123303.359313 LR: 1.00e-04\n",
      "Epoch 031 Train MSE: 120067.060844 Val MSE: 119974.021812 LR: 1.00e-04\n",
      "Epoch 032 Train MSE: 116775.569375 Val MSE: 116700.983688 LR: 1.00e-04\n",
      "Epoch 033 Train MSE: 113540.636594 Val MSE: 113481.720125 LR: 1.00e-04\n",
      "Epoch 034 Train MSE: 110369.795031 Val MSE: 110331.740625 LR: 1.00e-04\n",
      "Epoch 035 Train MSE: 107268.141781 Val MSE: 107253.657125 LR: 1.00e-04\n",
      "Epoch 036 Train MSE: 104239.592188 Val MSE: 104249.106813 LR: 1.00e-04\n",
      "Epoch 037 Train MSE: 101287.739844 Val MSE: 101322.623625 LR: 1.00e-04\n",
      "Epoch 038 Train MSE: 98417.923266 Val MSE: 98488.682750 LR: 1.00e-04\n",
      "Epoch 039 Train MSE: 95634.423125 Val MSE: 95731.168937 LR: 1.00e-04\n",
      "Epoch 040 Train MSE: 92940.759625 Val MSE: 93072.137188 LR: 1.00e-04\n",
      "Epoch 041 Train MSE: 90339.304484 Val MSE: 90496.727219 LR: 1.00e-04\n",
      "Epoch 042 Train MSE: 87833.241453 Val MSE: 88023.767063 LR: 1.00e-04\n",
      "Epoch 043 Train MSE: 85420.990516 Val MSE: 85642.315563 LR: 1.00e-04\n",
      "Epoch 044 Train MSE: 83101.678203 Val MSE: 83355.430625 LR: 1.00e-04\n",
      "Epoch 045 Train MSE: 80879.763875 Val MSE: 81167.504500 LR: 1.00e-04\n",
      "Epoch 046 Train MSE: 78753.640437 Val MSE: 79067.265750 LR: 1.00e-04\n",
      "Epoch 047 Train MSE: 76716.157562 Val MSE: 77060.984906 LR: 1.00e-04\n",
      "Epoch 048 Train MSE: 74766.129875 Val MSE: 75133.456219 LR: 1.00e-04\n",
      "Epoch 049 Train MSE: 72898.917641 Val MSE: 73286.052219 LR: 1.00e-04\n",
      "Epoch 050 Train MSE: 71112.440734 Val MSE: 71521.668594 LR: 1.00e-04\n",
      "Epoch 051 Train MSE: 69404.694219 Val MSE: 69834.246906 LR: 1.00e-04\n",
      "Epoch 052 Train MSE: 67775.788953 Val MSE: 68220.405125 LR: 1.00e-04\n",
      "Epoch 053 Train MSE: 66224.438469 Val MSE: 66697.502438 LR: 1.00e-04\n",
      "Epoch 054 Train MSE: 64752.541703 Val MSE: 65237.313000 LR: 1.00e-04\n",
      "Epoch 055 Train MSE: 63362.323812 Val MSE: 63871.804000 LR: 1.00e-04\n",
      "Epoch 056 Train MSE: 62056.035625 Val MSE: 62578.554781 LR: 1.00e-04\n",
      "Epoch 057 Train MSE: 60833.627297 Val MSE: 61379.501906 LR: 1.00e-04\n",
      "Epoch 058 Train MSE: 59696.881453 Val MSE: 60264.244500 LR: 1.00e-04\n",
      "Epoch 060 Train MSE: 57683.488000 Val MSE: 58298.058563 LR: 1.00e-04\n",
      "Epoch 061 Train MSE: 56807.887578 Val MSE: 57445.854188 LR: 1.00e-04\n",
      "Epoch 062 Train MSE: 56016.637359 Val MSE: 56675.988719 LR: 1.00e-04\n",
      "Epoch 063 Train MSE: 55310.223953 Val MSE: 55995.956906 LR: 1.00e-04\n",
      "Epoch 064 Train MSE: 54687.449375 Val MSE: 55390.132125 LR: 1.00e-04\n",
      "Epoch 065 Train MSE: 54145.166719 Val MSE: 54871.347250 LR: 1.00e-04\n",
      "Epoch 066 Train MSE: 53678.748531 Val MSE: 54424.029187 LR: 1.00e-04\n",
      "Epoch 067 Train MSE: 53283.558234 Val MSE: 54048.828406 LR: 1.00e-04\n",
      "Epoch 068 Train MSE: 52955.419656 Val MSE: 53732.159031 LR: 1.00e-04\n",
      "Epoch 069 Train MSE: 52688.072922 Val MSE: 53482.671875 LR: 1.00e-04\n",
      "Epoch 070 Train MSE: 52473.609078 Val MSE: 53276.155937 LR: 1.00e-04\n",
      "Epoch 071 Train MSE: 52305.931703 Val MSE: 53117.028219 LR: 1.00e-04\n",
      "Epoch 072 Train MSE: 52176.657266 Val MSE: 52995.105438 LR: 1.00e-04\n",
      "Epoch 073 Train MSE: 52080.177781 Val MSE: 52902.112125 LR: 1.00e-04\n",
      "Epoch 074 Train MSE: 52009.787953 Val MSE: 52834.218844 LR: 1.00e-04\n",
      "Epoch 075 Train MSE: 51958.451625 Val MSE: 52782.527844 LR: 1.00e-04\n",
      "Epoch 076 Train MSE: 51922.907500 Val MSE: 52747.461187 LR: 1.00e-04\n",
      "Epoch 077 Train MSE: 51898.367031 Val MSE: 52724.255344 LR: 1.00e-04\n",
      "Epoch 078 Train MSE: 51886.154422 Val MSE: 52707.058500 LR: 1.00e-04\n",
      "Epoch 079 Train MSE: 51872.084453 Val MSE: 52693.879000 LR: 1.00e-04\n",
      "Epoch 080 Train MSE: 51865.370484 Val MSE: 52686.069469 LR: 1.00e-04\n",
      "Epoch 081 Train MSE: 51860.795984 Val MSE: 52679.708469 LR: 1.00e-04\n",
      "Epoch 082 Train MSE: 51858.890953 Val MSE: 52675.511844 LR: 1.00e-04\n",
      "Epoch 083 Train MSE: 51855.884656 Val MSE: 52672.169719 LR: 1.00e-04\n",
      "Epoch 084 Train MSE: 51855.524953 Val MSE: 52671.805719 LR: 1.00e-04\n",
      "Epoch 085 Train MSE: 51856.304781 Val MSE: 52669.702500 LR: 1.00e-04\n",
      "Epoch 086 Train MSE: 51854.573781 Val MSE: 52668.664156 LR: 1.00e-04\n",
      "Epoch 087 Train MSE: 51853.985078 Val MSE: 52666.618875 LR: 1.00e-04\n",
      "Epoch 088 Train MSE: 51853.965187 Val MSE: 52666.387094 LR: 5.00e-05\n",
      "Epoch 089 Train MSE: 51851.795531 Val MSE: 52666.042000 LR: 5.00e-05\n",
      "Epoch 090 Train MSE: 51852.284453 Val MSE: 52665.678500 LR: 5.00e-05\n",
      "Epoch 091 Train MSE: 51851.550047 Val MSE: 52665.523844 LR: 5.00e-05\n",
      "Epoch 092 Train MSE: 51852.013047 Val MSE: 52665.306563 LR: 5.00e-05\n",
      "Epoch 094 Train MSE: 51852.410828 Val MSE: 52665.491969 LR: 5.00e-05\n",
      "Epoch 095 Train MSE: 51851.599359 Val MSE: 52666.178375 LR: 2.50e-05\n",
      "Epoch 096 Train MSE: 51851.999328 Val MSE: 52665.538875 LR: 2.50e-05\n",
      "Epoch 097 Train MSE: 51851.480984 Val MSE: 52665.652656 LR: 2.50e-05\n",
      "Epoch 098 Train MSE: 51852.036266 Val MSE: 52665.377406 LR: 2.50e-05\n",
      "Epoch 099 Train MSE: 51851.454875 Val MSE: 52665.153969 LR: 2.50e-05\n",
      "Epoch 100 Train MSE: 51851.270953 Val MSE: 52665.054406 LR: 2.50e-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DBzdtDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, seq_len=40, d_model=64, n_heads=4, n_layers=3, dim_ff=128, out_dim=39, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Linear(1, d_model)\n",
    "        self.pos_emb   = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_ff, dropout=dropout, activation=\"gelu\", batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.head    = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, out_dim))\n",
    "    def forward(self, x):\n",
    "        b, t = x.shape\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.token_emb(x)\n",
    "        x = x + self.pos_emb[:, :t, :]\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.head(x)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_ds = DBzdtDataset(X_train, y_train)\n",
    "test_ds  = DBzdtDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "model = TransformerRegressor()\n",
    "model.to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5, verbose=True)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = loss_fn(preds, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    sched.step(train_loss)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_loss += loss_fn(model(xb), yb).item() * xb.size(0)\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    print(f\"Epoch {epoch:03d} Train MSE: {train_loss:.6f} Val MSE: {val_loss:.6f} LR: {opt.param_groups[0]['lr']:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22ca58-c64a-4d38-b7c8-65e05e7ec99f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, skip=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.skip = skip\n",
    "        if skip and in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.skip:\n",
    "            out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
    "        self.layer1 = ResNetBlock(32, 64, skip=True)\n",
    "        self.layer2 = ResNetBlock(64, 64, skip=True)\n",
    "        self.layer3 = ResNetBlock(64, 128, skip=True)\n",
    "        self.layer4 = ResNetBlock(128, 128, skip=True)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, 39)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = self.input_proj(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model = ResNet()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    y_pred = model(X_train_t)\n",
    "    loss = loss_fn(y_pred, y_train_t)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(X_test_t)\n",
    "        loss_test = loss_fn(y_pred_test, y_test_t)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch:{epoch} | loss_train: {loss} | loss_test: {loss_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b26d35",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T06:56:51.362928Z",
     "iopub.status.idle": "2025-07-10T06:56:51.363275Z",
     "shell.execute_reply": "2025-07-10T06:56:51.363112Z",
     "shell.execute_reply.started": "2025-07-10T06:56:51.363097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.tensor(X, dtype=torch.float32)\n",
    "        y_pred = model(X_t)\n",
    "        return y_pred.numpy()\n",
    "\n",
    "print(predict(X_test[:1]))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7838674,
     "sourceId": 12427550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
